---
title: "K-Nearest Neighbors Notes"
date: 2019-05-13
permalink: /notes/2019/05/13/knns
--- 

This is our first true classification algorithm. It is extremely simple and a great starter.

#### Theory
We want to classify a set of inputs into certain categories. Hence, we have categories and the features associated with them. We do this by finding the nearest k data points and classify the new input as the most common of class of the k closest data points.

#### Advantages
- Very simple, easy to understand
- No training! Training consists of just obtaining data itself

#### Disadvantages
- Can be very slow (~O(n^2))

#### Implementation
```python
>>> import numpy as np
>>> from sklearn import preprocessing, model_selection, neighbors
>>> X = np.array([[1, 2], [2, 3], [3, 1], [6, 5], [7, 7], [8, 6]])
>>> X.shape
(6, 2)
>>> y = np.array([0, 0, 0, 1, 1, 1])
>>> y.shape
(6,)
>>> X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)
>>> clf = neighbors.KNeighborsClassifier(n_neighbors=3)
>>> clf.fit(X_train, y_train)
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
metric_params=None, n_jobs=None, n_neighbors=3, p=2,
weights='uniform')
>>> accuracy = clf.score(X_test, y_test)
>>> print(accuracy)
0.0 # expected since we have a small dataset
>>> example_measures = np.array([5, 6])
>>> example_measures = example_measures.reshape(-1, len(example_measures))
>>> prediction = clf.predict(example_measures)
>>> print(prediction)
0 # expected since we have a small dataset
```
