---
title: "Support Vector Machines - Hard Margin"
date: 2019-05-17
permalink: /notes/2019/05/17/svms
--- 
A binary classifier - this means the goal of the SVM is to find the best separating hyperplane or decision boundary between two classes. It achieves this by finding the hyperplane that maximizes the perpendicular distance from the nearest data points. 

#### How are new inputs classified?
If we have a decision boundary, we can define it using the vector perpendicular to decision boundary `w`. We also have an unknown data point `u`. 

If `u dot w + b > 0`, then we have one class and if `u dot w + b < 0` we have the other class.

If `u dot w + b = 0`, we are on the decision boundary!

#### How is the hyperplane calculated?
First, let us define what a support vector is. Support vectors are vectors that if we move it, it would affect the best separating hyperplane. 

The SVM maximize the width or distance between the two hyperplanes going through the supporting vectors.

![alt_text](https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/800px-SVM_margin.png)
*Here, the 2 blue dots and the 1 green dot are the support vectors*

Thus, we have the hinge loss cost function:
```
c(x, y, f(x)) = max(0, 1-y*f(x)) = max(0, 1-y*(w*x))
# If we classify properly, then the cost is 0, otherwise the cost is 1
```

We add the regularization parameter:
```
c(x, y, w) = lambda/2*||w||^2 + sum_i=1_to_n(1-y_i*(w*x_i))
```
Taking derivatives:
```
d c(x, y, w)/d(w) = lambda*||w||+sum_i=1_to_n(-y_i*x_i)
```

So the update rule is:
```
if 1-y*x(w*x)>=0: # incorrect
  w_t = w_{t-1} - alpha * (lambda*||w||-y*x)  
else:
  w_t = w_{t-1} - alpha * (lambda*||w||
```


### Advantages
- Uses regularization to avoid overfitting
- Uses kernel trick to increase speed

### Disadvantages
- Difficult interpretation
- Choosing a good kernel function is hard

### Implementation
```python
>>> import numpy as np
>>> from sklearn import model_selection, svm
>>> X = np.array([[1, 2], [2, 3], [3, 1], [6, 5], [7, 7], [8, 6]])
>>> y = np.array([-1, -1, -1, 1, 1, 1])
>>> X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)
>>> clf = svm.SVC()
>>> clf.fit(X_train, y_train)
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
"avoid this warning.", FutureWarning)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
kernel='rbf', max_iter=-1, probability=False, random_state=None,
shrinking=True, tol=0.001, verbose=False)
>>> accuracy = clf.score(X_test, y_test)
>>> print(accuracy)
1.0
>>> example_measures = np.array([5, 6])
>>> example_measures = example_measures.reshape(-1, len(example_measures))
>>> prediction = clf.predict(example_measures)
>>> print(prediction)
```

